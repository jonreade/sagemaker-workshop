{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Classification with Tensorflow on Amazon SageMaker - Directly in your notebook\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "3. [Train the model](#Train-the-model)\n",
    "4. [Test the model](#Test-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Image classification is an increasingly popular machine learning technique, in which a trained model predicts which of several classes is represented by a particular image. This technique is useful across a wide variety of use cases from manufacturing quality control to medical diagnosis. To create an image classification solution, we need to acquire and process an image dataset, and train a model from that dataset. The trained model is then capable of identifying features and predicting which class an image belongs to. Finally, we can make predictions using the trained model against previously unseen images.\n",
    "\n",
    "This notebook is an end-to-end example showing how to build a custom image classifier using TensorFlow and Keras, simply using Amazon SageMaker's hosted Jupyter notebook directly. This is an easy transition from traditional machine learning development you may already be doing on your laptop or on an Amazon EC2 instance. Subsequent notebooks in this workshop will demonstrate how to take full advantage of SageMaker's training service, hosting service, and automatic model tuning. Note that for complex large scale machine learning models, training directly in a notebook can be cost prohibitive.\n",
    "\n",
    "For each of the labs in this workshop, we use a publicly available set of bird images based on the [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset. We demonstrate transfer learning by leveraging pretrained ImageNet weights. In this example, we use a MobileNet V2 network architecture, but you can choose any of dozens of available pretrained models found (here)[https://keras.io/api/applications/], by just changing one line of code.\n",
    "\n",
    "For a quick demonstration, pick a small handful of bird species (set `SAMPLE_ONLY = True` and choose a few classes / species). For a more complete model, you can train against all 200 bird species in the dataset. For anything more than a few classes, be sure to upgrade your notebook instance type to one of SageMaker's GPU instance types (e.g., ml.p2, ml.p3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset contains 11,788 images across 200 bird species (the original technical report can be found [here](http://www.vision.caltech.edu/visipedia/papers/CUB_200_2011.pdf)).  Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels.  Bounding boxes are provided, as are annotations of bird parts.  A recommended train/test split is given, but image size data is not.\n",
    "\n",
    "![](./cub_200_2011_snapshot.png)\n",
    "\n",
    "The dataset can be downloaded [here](https://course.fast.ai/datasets). Note that the file size is around 1.2 GB, and can take many minutes to download depending on your network connection. As an alternative, we have included `CUB_MINI.tar` in this workshop repo. It contains images and metadata for 8 of the 200 species, and can be used effectively for testing and learning how to use SageMaker and TensorFlow together on computer vision use cases.\n",
    "\n",
    "### Download and unpack the full dataset\n",
    "\n",
    "Here we download the birds dataset from CalTech. You can do this once and keep the unpacked dataset in your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split('/')[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# download('http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz')\n",
    "# CalTech's download is (at least temporarily) unavailable since August 2020. \n",
    "\n",
    "# Can now use one made available by fast.ai .\n",
    "# download ('https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Clean up prior version of the downloaded dataset if you are running this again\n",
    "# !rm -rf CUB_200_2011  \n",
    "\n",
    "# # Unpack and then remove the downloaded compressed tar file\n",
    "# !gunzip -c ./CUB_200_2011.tgz | tar xopf - \n",
    "# !rm CUB_200_2011.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack the small subset of 8 bird species (CUB_MINI.tar)\n",
    "Here we unpack the small dataset, included with the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvf CUB_MINI.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some parameters for the rest of the notebook to use\n",
    "Here we define a few parameters that help drive the rest of the notebook.  For example, `SAMPLE_ONLY` is defaulted to `True`. This will force the notebook to train on only a handful of species.  Setting `SAMPLE_ONLY` to false will make the notebook work with the entire dataset of 200 bird species.  This makes the training a more difficult challenge, and you will need to tune parameters and run more epochs.\n",
    "\n",
    "An `EXCLUDE_IMAGE_LIST` is defined as a mechanism to address any corrupt images from the dataset and ensure they do not disrupt the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To speed up training and experimenting, you can use a small handful of species.\n",
    "# To see the full list of the classes available, look at the content of CLASSES_FILE.\n",
    "SAMPLE_ONLY  = True\n",
    "CLASSES = [13, 17] #, 35, 36, 47, 68, 73, 87]\n",
    "\n",
    "# Otherwise, you can use the full set of species\n",
    "if (not SAMPLE_ONLY):\n",
    "    CLASSES = []\n",
    "    for c in range(200):\n",
    "        CLASSES += [c + 1]\n",
    "\n",
    "BASE_DIR   = 'CUB_MINI/' # or use 'CUB_200_2011/' if you downloaded the full dataset\n",
    "IMAGES_DIR = BASE_DIR + 'images/'\n",
    "\n",
    "CLASSES_FILE = BASE_DIR + 'classes.txt'\n",
    "IMAGE_FILE   = BASE_DIR + 'images.txt'\n",
    "LABEL_FILE   = BASE_DIR + 'image_class_labels.txt'\n",
    "\n",
    "SPLIT_RATIOS = (0.7, 0.2, 0.1)\n",
    "\n",
    "CLASS_COLS      = ['class_number','class_id']\n",
    "\n",
    "EXCLUDE_IMAGE_LIST = ['087.Mallard/Mallard_0130_76836.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the dataset\n",
    "Show the list of bird species or dataset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.read_csv(CLASSES_FILE, sep=' ', names=CLASS_COLS, header=None)\n",
    "criteria = classes_df['class_number'].isin(CLASSES)\n",
    "classes_df = classes_df[criteria]\n",
    "\n",
    "class_name_list = sorted(classes_df['class_id'].unique().tolist())\n",
    "print(class_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each species, there are dozens of images of various shapes and sizes. By dividing the entire dataset into individual named (numbered) folders, the images are in effect labelled for supervised learning using image classification and object detection algorithms. \n",
    "\n",
    "The following function displays a grid of thumbnail images for all the image files for a given species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_species(species_id):\n",
    "    _im_list = !ls $IMAGES_DIR/$species_id\n",
    "\n",
    "    NUM_COLS = 4\n",
    "    IM_COUNT = len(_im_list)\n",
    "\n",
    "    print('Species ' + species_id + ' has ' + str(IM_COUNT) + ' images.')\n",
    "    \n",
    "    NUM_ROWS = int(IM_COUNT / NUM_COLS)\n",
    "    if ((IM_COUNT % NUM_COLS) > 0):\n",
    "        NUM_ROWS += 1\n",
    "\n",
    "    fig, axarr = plt.subplots(NUM_ROWS, NUM_COLS)\n",
    "    fig.set_size_inches(12.0, 20.0, forward=True)\n",
    "\n",
    "    curr_row = 0\n",
    "    for curr_img in range(IM_COUNT):\n",
    "        # fetch the url as a file type object, then read the image\n",
    "        f = IMAGES_DIR + species_id + '/' + _im_list[curr_img]\n",
    "        a = plt.imread(f)\n",
    "\n",
    "        # find the column by taking the current index modulo 3\n",
    "        col = curr_img % NUM_ROWS\n",
    "        # plot on relevant subplot\n",
    "        axarr[col, curr_row].imshow(a)\n",
    "        if col == (NUM_ROWS - 1):\n",
    "            # we have finished the current row, so increment row counter\n",
    "            curr_row += 1\n",
    "    fig.tight_layout()       \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_species('017.Cardinal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train/val/test dataframes from our dataset\n",
    "Here we split our dataset into training, testing, and validation datasets, each in their own Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split\n",
    "train_df, val_df, test_df = split.get_train_val_dataframes(BASE_DIR, CLASSES, EXCLUDE_IMAGE_LIST, SPLIT_RATIOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model \n",
    "In this section of the notebook, we train an image classification model to predict the bird species given an image. In many cases, you are able to leverage a technique called [transfer learning](https://www.tensorflow.org/tutorials/images/transfer_learning), which uses pretrained models to dramatically simplify the process. Highly accurate classification models can be built using relatively small datasets and very few epochs, since you are starting with pretrained weights. In this notebook, we use [pretrained models from Keras](https://keras.io/api/applications/). Dozens of pretrained image classification models are available (ResNet, Inception, DenseNet, ...) with varying accuracy and model size. MobileNetV2 is often used due it's small size (only 14 MB) to help with deployment to constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT     = 224\n",
    "WIDTH      = 224\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(f'Using TensorFlow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "LAST_FROZEN_LAYER = 20\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare image data generators from our dataframes\n",
    "In this section, we use Tensorflow's [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class to give a consistent way to access batches of our training, testing, and validation images. Tensorflow training will use these generators to pull sets of images as it makes its way through each training epoch. Random adjustments are made to image brightness, rotation, width, and height, and some images will be flipped along the horizontal axis (for a bird facing left, provide an equivalent image with the bird facing to the right instead). These adjustments are called image augmentations, and they help avoid the trained model perform poorly on previously unseen images (known as overfitting).\n",
    "\n",
    "Here is a [tutorial on transfer learning using TensorFlow](https://www.tensorflow.org/tutorials/images/transfer_learning) for those that want additional explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen =  ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input,\n",
    "      rotation_range=60,\n",
    "      brightness_range=(0.8, 1.0),\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=False\n",
    "    )\n",
    "val_datagen  = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(train_df, directory=IMAGES_DIR,\n",
    "                                              x_col='image_file_name', y_col='class_id',\n",
    "                                              target_size=(HEIGHT, WIDTH), \n",
    "                                              batch_size=BATCH_SIZE)\n",
    "val_gen = train_datagen.flow_from_dataframe(val_df, directory=IMAGES_DIR,\n",
    "                                              x_col='image_file_name', y_col='class_id',\n",
    "                                              target_size=(HEIGHT, WIDTH), \n",
    "                                              batch_size=BATCH_SIZE)\n",
    "test_gen = train_datagen.flow_from_dataframe(test_df, directory=IMAGES_DIR,\n",
    "                                              x_col='image_file_name', y_col='class_id',\n",
    "                                              target_size=(HEIGHT, WIDTH), \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False) # need predictable order for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "Here we use the pretrained model as a base, and add our own classification layer that lets our model predict bird species. We flag the pretrained layers to not be trainable, and add one or more dense fully connected layers with dropout, which also helps avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(HEIGHT, WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    # Freeze all base layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        x = Dense(fc, activation='relu')(x) \n",
    "        if (dropout != 0.0):\n",
    "            x = Dropout(dropout)(x)\n",
    "\n",
    "    # New softmax layer\n",
    "    predictions = Dense(num_classes, activation='softmax', name='output')(x) \n",
    "    \n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return finetune_model\n",
    "\n",
    "# Here we extend the base model with additional fully connected layers, dropout for avoiding\n",
    "# overfitting to the training dataset, and a classification layer\n",
    "num_classes = len(class_name_list)\n",
    "model = build_finetune_model(base_model, \n",
    "                              dropout=0.5, \n",
    "                              fc_layers=[1024], \n",
    "                              num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training\n",
    "Now that the model has been created, it is time for training. We use some initial epochs to train the model up to 90% accuracy. We then perform fine tuning of the weights of the top layers of the pretrained model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "INITIAL_EPOCHS = 2\n",
    "\n",
    "num_train_images = len(train_gen.filepaths)\n",
    "num_val_images   = len(val_gen.filepaths)\n",
    "\n",
    "opt = RMSprop(lr=0.00001) # or Adam\n",
    "\n",
    "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_gen, epochs=INITIAL_EPOCHS, workers=8, \n",
    "                                   steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                   validation_data=val_gen, validation_steps=num_val_images // BATCH_SIZE,\n",
    "                                   shuffle=True)\n",
    "\n",
    "for layer in model.layers[LAST_FROZEN_LAYER:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit_generator(train_gen, epochs=NUM_EPOCHS, workers=8, \n",
    "                                   steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                   validation_data=val_gen, validation_steps=num_val_images // BATCH_SIZE,\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy across epochs\n",
    "Here we show the training and validation accuracy across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'r.')\n",
    "    plt.plot(epochs, val_acc, 'r')\n",
    "    plt.title('Training accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig('acc_vs_epochs.png')\n",
    "    \n",
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_preds = model.evaluate_generator(test_gen, steps=test_df.shape[0])\n",
    "print('Loss: {:.2f}, Accuracy: {:.2f}'.format(eval_preds[0], eval_preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "In this section, we test the model against our validation and test sets, identifying accuracy, displaying images we are getting wrong, and summarizing model accuracy with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "def predict_bird_from_file(fn, verbose=True):\n",
    "    img = image.load_img(fn, target_size=(HEIGHT, WIDTH))\n",
    "    x = image.img_to_array(img)\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    results = model.predict(x)\n",
    "    predicted_class_idx = argmax(results)\n",
    "    predicted_class = class_name_list[predicted_class_idx]\n",
    "    confidence = results[0][predicted_class_idx]\n",
    "    if verbose:\n",
    "        display(img)\n",
    "        print('Class: {}, confidence: {:.2f}'.format(predicted_class, confidence))\n",
    "    del img, x\n",
    "    return predicted_class_idx, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = IMAGES_DIR + '/' + test_df.iloc[0]['image_file_name']\n",
    "predict_bird_from_file(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), \n",
    "                                  range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.gca().set_xticklabels(class_name_list)\n",
    "    plt.gca().set_yticklabels(class_name_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def create_and_plot_confusion_matrix(actual, predicted):\n",
    "    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(class_name_list)))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=range(len(class_name_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess prediction performance against validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Iterate through entire dataframe, tracking predictions and accuracy. For mistakes, show the image, and the predicted and actual classes to help understand\n",
    "# where the model may need additional tuning.\n",
    "\n",
    "def test_image_df(df):\n",
    "    print('Testing {} images'.format(df.shape[0]))\n",
    "    num_errors = 0\n",
    "    preds = []\n",
    "    acts  = []\n",
    "    for i in range(df.shape[0]):\n",
    "        fname = df.iloc[i]['image_file_name']\n",
    "        act   = int(df.iloc[i]['class_id']) - 1\n",
    "        acts.append(act)\n",
    "        pred, conf = predict_bird_from_file(IMAGES_DIR + '/' + fname, verbose=False)\n",
    "        preds.append(pred)\n",
    "        if (pred != act):\n",
    "            num_errors += 1\n",
    "            print('ERROR on image index {} -- Pred: {} {:.2f}, Actual: {}'.format(i, \n",
    "                                                                   class_name_list[pred], conf, \n",
    "                                                                   class_name_list[act]))\n",
    "            img = Image(filename=f'{IMAGES_DIR}/{fname}', width=WIDTH, height=HEIGHT)\n",
    "            display(img)\n",
    "    return num_errors, preds, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = val_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(val_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = test_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(test_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model against previously unseen images\n",
    "Lastly, we download images that the algorithm has not yet seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O northern-cardinal-2.jpg https://www.allaboutbirds.org/guide/assets/photo/63667291-480px.jpg\n",
    "!wget -q -O bobolink.jpg https://upload.wikimedia.org/wikipedia/commons/9/9a/Bobolink_at_Lake_Woodruff_-_Flickr_-_Andrea_Westmoreland_%281%29.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bird_from_file('northern-cardinal-2.jpg')\n",
    "predict_bird_from_file('bobolink.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
